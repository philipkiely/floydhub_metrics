{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#Setup A\n",
    "actual_a = [1 for n in range(10)] + [0 for n in range(10)]\n",
    "predicted_a = [1 for n in range(9)] + [0, 1, 1] + [0 for n in range(8)]\n",
    "print(actual_a)\n",
    "print(predicted_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "#Setup B\n",
    "actual_b = [1 for n in range(10)] + [2 for n in range(10)] + [3 for n in range(10)]\n",
    "predicted_b = [1 for n in range(8)] + [2, 3] + [2 for n in range(8)] + [1, 1] + [3 for n in range(9)] + [2]\n",
    "print(actual_b)\n",
    "print(predicted_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9]\n",
      "[0, 0, 0, 1, 0, 1, 2, 1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5, 6, 5, 6, 7, 6, 7, 8, 7, 8, 9, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "#Setup C\n",
    "actual_c = [c for m in [[n, n, n] for n in range(10)] for c in m]\n",
    "predicted_c = [c for m in [[n, max((n-1), 0), n] for n in range(10)] for c in m]\n",
    "print(actual_c)\n",
    "print(predicted_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my Confusion Matrix A:\n",
      " [[8 2]\n",
      "  [1 9]]\n",
      "sklearn Confusion Matrix A:\n",
      " [[8 2]\n",
      " [1 9]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def my_confusion_matrix(actual, predicted):\n",
    "    true_positives = len([a for a, p in zip(actual, predicted) if a == p and p == 1])\n",
    "    true_negatives = len([a for a, p in zip(actual, predicted) if a == p and p == 0])\n",
    "    false_positives = len([a for a, p in zip(actual, predicted) if a != p and p == 1])\n",
    "    false_negatives = len([a for a, p in zip(actual, predicted) if a != p and p == 0])\n",
    "    return \"[[{} {}]\\n  [{} {}]]\".format(true_negatives, false_positives, false_negatives, true_positives)\n",
    "\n",
    "print(\"my Confusion Matrix A:\\n\", my_confusion_matrix(actual_a, predicted_a))\n",
    "print(\"sklearn Confusion Matrix A:\\n\", confusion_matrix(actual_a, predicted_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my Accuracy A: 0.85\n",
      "sklearn Accuracy A: 0.85\n",
      "my Accuracy B: 0.8333333333333334\n",
      "sklearn Accuracy B: 0.8333333333333334\n",
      "my Accuracy C: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Accuracy = TP + TN / TP + TN + FP + FN\n",
    "def my_accuracy_score(actual, predicted): #threshold for non-classification?  \n",
    "    true_positives = len([a for a, p in zip(actual, predicted) if a == p and p == 1])\n",
    "    true_negatives = len([a for a, p in zip(actual, predicted) if a == p and p == 0])\n",
    "    false_positives = len([a for a, p in zip(actual, predicted) if a != p and p == 1])\n",
    "    false_negatives = len([a for a, p in zip(actual, predicted) if a != p and p == 0])\n",
    "    return (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "\n",
    "# Accuracy for non-binary predictions\n",
    "def my_general_accuracy_score(actual, predicted):\n",
    "    correct = len([a for a, p in zip(actual, predicted) if a == p])\n",
    "    wrong = len([a for a, p in zip(actual, predicted) if a != p])\n",
    "    return correct / (correct + wrong)\n",
    "\n",
    "# Accuracy for continuous with threshold\n",
    "def my_threshold_accuracy_score(actual, predicted, threshold):\n",
    "    a = [0 if x >= threshold else 1 for x in actual]\n",
    "    p = [0 if x >= threshold else 1 for x in predicted]\n",
    "    return my_accuracy_score(a, p)\n",
    "\n",
    "print(\"my Accuracy A:\", my_accuracy_score(actual_a, predicted_a))\n",
    "print(\"sklearn Accuracy A:\", accuracy_score(actual_a, predicted_a))\n",
    "print(\"my Accuracy B:\", my_general_accuracy_score(actual_b, predicted_b))\n",
    "print(\"sklearn Accuracy B:\", accuracy_score(actual_b, predicted_b))\n",
    "print(\"my Accuracy C:\", my_threshold_accuracy_score(actual_c, predicted_c, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my Precision A: 0.8181818181818182\n",
      "sklearn Precision A: 0.8181818181818182\n",
      "my Precision B: 0.8\n"
     ]
    }
   ],
   "source": [
    "#Precision\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Precision = TP / TP + FP\n",
    "def my_precision_score(actual, predicted):\n",
    "    true_positives = len([a for a, p in zip(actual, predicted) if a == p and p == 1])\n",
    "    false_positives = len([a for a, p in zip(actual, predicted) if a != p and p == 1])\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def my_general_precision_score(actual, predicted, value):\n",
    "    true_positives = len([a for a, p in zip(actual, predicted) if a == p and p == value])\n",
    "    false_positives = len([a for a, p in zip(actual, predicted) if a != p and p == value])\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "print(\"my Precision A:\", my_precision_score(actual_a, predicted_a))\n",
    "print(\"sklearn Precision A:\", precision_score(actual_a, predicted_a))\n",
    "print(\"my Precision B:\", my_general_precision_score(actual_b, predicted_b, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my Recall A: 0.9\n",
      "sklearn Recall A: 0.9\n"
     ]
    }
   ],
   "source": [
    "#Recall\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def my_recall_score(actual, predicted):\n",
    "    true_positives = len([a for a, p in zip(actual, predicted) if a == p and p == 1])\n",
    "    false_negatives = len([a for a, p in zip(actual, predicted) if a != p and p == 0])\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "def my_general_recall_score(actual, predicted, positive, negative):\n",
    "    true_positives = len([a for a, p in zip(actual, predicted) if a == p and p == positive])\n",
    "    false_negatives = len([a for a, p in zip(actual, predicted) if a != p and p == negative])\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "print(\"my Recall A:\", my_recall_score(actual_a, predicted_a))\n",
    "print(\"sklearn Recall A:\", recall_score(actual_a, predicted_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my F1 Score A: 0.8571428571428572\n",
      "sklearn F1 Score A: 0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "#F1 Score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def my_f1_score(actual, predicted):\n",
    "    return 2 * (my_precision_score(actual, predicted) * my_recall_score(actual, predicted)) / (my_precision_score(actual, predicted) + my_recall_score(actual, predicted))\n",
    "\n",
    "print(\"my F1 Score A:\", my_f1_score(actual_a, predicted_a))\n",
    "print(\"sklearn F1 Score A:\", f1_score(actual_a, predicted_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(actual_a, predicted_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
